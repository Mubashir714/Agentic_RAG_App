{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwGgF86_HM_x"
      },
      "source": [
        "#Install Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCDtwj1oHBNN"
      },
      "outputs": [],
      "source": [
        "!pip install langchain openai pinecone-client fastapi uvicorn python-dotenv\n",
        "!pip install langchain-community\n",
        "!pip install --upgrade pinecone-client\n",
        "!pip install huggingface_hub\n",
        "!pip install datasets\n",
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFH1yTJVHbQG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.vectorstores import Pinecone\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from datasets import load_dataset\n",
        "from dotenv import load_dotenv\n",
        "from pinecone import pinecone, ServerlessSpec"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setting up API Keys for integrating with vector database and creating embedding vectors."
      ],
      "metadata": {
        "id": "3EEr5hyDJ6XC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAvxYmj5V_Se"
      },
      "outputs": [],
      "source": [
        "# Define environment variables content\n",
        "env_content = \"\"\"\n",
        "OPENAI_API_KEY=\"Your OPEN API KEY\"\n",
        "PINECONE_API_KEY=\"Your API KEY\"\n",
        "PINECONE_ENV=\" Your PINECONE ENV\"\n",
        "\"\"\"\n",
        "\n",
        "# Create and write to the .env file\n",
        "with open('.env', 'w') as f:\n",
        "    f.write(env_content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oq_cvxQIIe-I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21ffef9d-4edd-4b00-9e6c-d643cd5afc9a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()  # Loads environment variables from .env file\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load environment variables"
      ],
      "metadata": {
        "id": "q-QWyMdkKTYT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYX_xxVDIj1X"
      },
      "outputs": [],
      "source": [
        "\n",
        "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")  # Add your Pinecone API key\n",
        "if not PINECONE_API_KEY:\n",
        "    raise ValueError(\"Key is missing. Please set it in your environment variables.\")\n",
        "\n",
        "PINECONE_ENV = os.getenv(\"PINECONE_ENV\")          # Add your Pinecone environment\n",
        "\n",
        "INDEX_NAME = \"agenticrag\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load OpenAI API Key"
      ],
      "metadata": {
        "id": "57VPqZ5KKYUS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOhJPWu5N6t4"
      },
      "outputs": [],
      "source": [
        "# Initialize FastAPI\n",
        "app = FastAPI()\n",
        "\n",
        "# Load OpenAI API Key\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "if not OPENAI_API_KEY:\n",
        "    raise ValueError(\"OpenAI API key is missing. Please set it in your environment variables.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize embeddings and vector store"
      ],
      "metadata": {
        "id": "xx_DfTdsKa8H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4cF81m0So2C"
      },
      "outputs": [],
      "source": [
        "\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
        "vector_store = Pinecone.from_existing_index(index_name=INDEX_NAME, embedding=embeddings)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize LLM and memory"
      ],
      "metadata": {
        "id": "MxoH24MfKdw7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxHkL6h8XL4d"
      },
      "outputs": [],
      "source": [
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4\", temperature=0, openai_api_key=OPENAI_API_KEY)\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build the retrieval-augmented chain\n"
      ],
      "metadata": {
        "id": "aKay9fCXKkMc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmKzMg_kXTpm"
      },
      "outputs": [],
      "source": [
        "qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    retriever=vector_store.as_retriever(),\n",
        "    memory=memory,\n",
        "    return_source_documents=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0zkkdioXXwi"
      },
      "outputs": [],
      "source": [
        "@app.post(\"/query/\")\n",
        "async def query_agent(query: str):\n",
        "    \"\"\"\n",
        "    Endpoint to interact with the Agentic RAG system.\n",
        "    :param query: User's legal query.\n",
        "    :return: Response from the system.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = qa_chain.run(query)\n",
        "        return {\"response\": response}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "# Root endpoint\n",
        "@app.get(\"/\")\n",
        "def read_root():\n",
        "    return {\"message\": \"Welcome to the Agentic RAG Legal Assistant!\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the USA Constitution dataset"
      ],
      "metadata": {
        "id": "6uJiq8dqKqZq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyOuI-bKb_aA"
      },
      "outputs": [],
      "source": [
        "\n",
        "dataset = load_dataset(\"c4lliope/us-congress\")  # Replace with the dataset you have\n",
        "print(dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize Pinecone with API key and environment"
      ],
      "metadata": {
        "id": "5MV-6dMcKsNk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxV7VzXWiTdr"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pinecone import Pinecone\n",
        "\n",
        "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
        "INDEX_NAME = pc.Index(\"agenticrag\")\n",
        "\n",
        "# Initialize OpenAI embeddings\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# Set up the vector store\n",
        "vector_store = Pinecone(index_name=INDEX_NAME, embedding=embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensure 'chunks' is a list of strings (text data)"
      ],
      "metadata": {
        "id": "Dp1I2_EVKx4X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g068zTmrkmci"
      },
      "outputs": [],
      "source": [
        "\n",
        "chunks = dataset['train']['text']  # Replace with your relevant text field in the dataset\n",
        "\n",
        "# Make sure all elements in chunks are strings\n",
        "chunks = [str(text) for text in chunks]  # Convert any non-string data to string\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate embeddings for the text chunks (documents) using embed_documents\n"
      ],
      "metadata": {
        "id": "VhR2myYkK07d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_vectors = embeddings.embed_documents(chunks)\n"
      ],
      "metadata": {
        "id": "9AkEo_Oqsdz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare data for upsert (Pinecone expects tuples of (ID, vector, metadata))"
      ],
      "metadata": {
        "id": "qcvEJ7DIK45J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pinecone_data = [(str(i), embedding_vectors[i], {\"text\": chunks[i]}) for i in range(len(chunks))]\n"
      ],
      "metadata": {
        "id": "VJyhwy9KLb1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upsert the embeddings into Pinecone"
      ],
      "metadata": {
        "id": "jKcFn3mMK7Bz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "index.upsert(vectors=pinecone_data)"
      ],
      "metadata": {
        "id": "DRJ9Yb9mLfEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Query to find similar documents\n",
        "query = \"What is the preamble of the USA Constitution?\"\n",
        "\n",
        "# Generate the query embedding\n",
        "query_embedding = embeddings.embed(query)\n",
        "\n",
        "# Query Pinecone for the most similar documents\n",
        "results = index.query([query_embedding], top_k=3, include_metadata=True)\n",
        "\n",
        "# Print the results (showing the most relevant documents)\n",
        "for match in results['matches']:\n",
        "    print(f\"Score: {match['score']}, Text: {match['metadata']['text']}\")"
      ],
      "metadata": {
        "id": "bnASSrxALjBC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}